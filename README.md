# House_damage_detect

the datasets download address is: https://www.kaggle.com/datasets/kmader/satellite-images-of-hurricane-damage

Just use your account of Kaggle to download

This code use Pytorch program library to complete this project, which include:
--
import torch

from torchvision import transforms, datasets, models

import torch.nn as nn

import numpy as np

import cv2

import pandas as pd

from PIL import Image

from torch.utils.data import Dataset, DataLoader

import matplotlib.pyplot as plt

Prepare for train and val datasets
--

In order to make the model more generalized, we need to preprocess the image, which includes random cropping, horizontal flipping, and rotation.

Use the ResNet50 as the model
--
We use the pretained model, and take careful for freeze the model weight.

Futhermore, the target of this project is to detect if 'House' in the picture is damaged, it is essentially a binary classfication model. Thus we should modify the number of feature in "fn" layer 2048->512->2(dim = 1), and to avoid overfitting set drop node rate is 20%.

Use the Grad-CAM to create damage heatmap
--
![image](https://github.com/Alen-Xue/House_damage_detect/assets/126217366/b5150739-c632-4636-8cda-64c941967b90)

Add the damge score:

The more severe the damage, the lower the score, and conversely the higher the score

![image](https://github.com/Alen-Xue/House_damage_detect/assets/126217366/b5150739-c632-4636-8cda-64c941967b90)


Grad-CAM (Gradient-weighted Class Activation Mapping) is a technique that provides insight into the reasons behind neural network decisions. Specifically, it generates a heatmap of “class-discriminative” regions, allowing us to visually identify which parts of an image contribute the most (or least) to a given output classification.

For a task such as detecting damage in pictures, Grad-CAM can highlight areas that the model finds most indicative of damage. Here’s a step-by-step explanation of how it works:

	1.	Forward Pass: An image is passed through the neural network to obtain a prediction.
	2.	Target Layer: You select a layer from the network, typically a convolutional layer. This layer contains spatial information about the image. The deeper the layer, the higher the level of abstraction it will represent.
	3.	Compute Gradients: Perform a backward pass from the predicted class score (or from any chosen output neuron) to the selected layer’s output. The goal is to compute the gradients of the predicted class score with respect to the feature maps of our chosen layer.
	4.	Global Average Pooling (GAP): The gradients computed from the previous step represent the importance of each feature map for the predicted class. By performing GAP on these gradients, you get a series of weight coefficients (one for each feature map).
	5.	Weighted Combination: Multiply each feature map of the selected layer by its associated weight coefficient from the previous step.
	6.	ReLU Activation: Take the weighted combination from the previous step and pass it through a ReLU activation function. This ensures that you consider only the features that have a positive influence on the predicted class, effectively removing negative pixels from the heatmap.
	7.	Upsampling: Resize the heatmap to the size of the input image.
	8.	Overlay on Original Image: For visualization purposes, overlay the heatmap on the original image. The resulting image highlights regions where the network thinks the damage exists.

In the context of damage detection:

	•	If the model has been trained effectively to identify damage in pictures, the heatmap generated by Grad-CAM should overlay the damage areas, suggesting that these regions highly influenced the model’s decision.
	•	The more intense the color in the heatmap (typically shown in red), the higher the model’s confidence that the corresponding area has damage.

Remember that while Grad-CAM can give an intuitive understanding of which areas the model is focusing on, it doesn’t provide an absolute measure of the model’s accuracy or reliability. If the model itself isn’t accurate or if it has been trained on poorly labeled data, the Grad-CAM visualizations might not correctly highlight damaged areas.

Use deeper layer model ResNet101 and add attention function.
--
Merge the ResNet101 and attention function (SE-block) can make model pay more attention on the feature areas.

After add attention function:
![image](https://github.com/Alen-Xue/House_damage_detect/assets/126217366/18e11cce-3e44-4feb-8130-1268d1a17d2a)

Add Transformer part to increase ability of model to detect the damage part
--

Adding Transformer to a traditional convolutional neural network, such as ResNet101, brings the following improvements and enhancements:

Better long-range dependency modeling: while traditional convolutional layers capture local information, the Transformer is specifically designed to capture long-range dependencies. By incorporating the Transformer among ResNet, the model can utilize both local features (provided by the convolutional layer) and global context (provided by the Transformer).

Adaptive receptive fields: As opposed to a fixed-size convolutional kernel, the Transformer can dynamically adapt its receptive fields, which allows the model to trade off features based on the complexity of the input data.

Better performance: On certain tasks, a convolutional model incorporating the Transformer may achieve better performance. For example, ViT (Vision Transformer) has demonstrated similar or better performance than state-of-the-art convolutional models on image classification tasks.

Better expressive power: Transformer's self-attention mechanism captures complex patterns and relationships, which enhances the expressive power of the model.

End-to-end attentional visualization: Transformer's attentional mechanism provides us with a tool to intuitively understand how the model builds relationships between inputs, which is beneficial for model interpretation and visualization.

However, introducing Transformer into ResNet101 has its challenges and tradeoffs:

Computational cost: Transformers are typically more computationally intensive than convolutional layers, especially when the input size is large. This can lead to slower training and inference.

Memory usage: Transformer's self-attention mechanism requires storing attentional weights, which can lead to out-of-memory problems on large datasets.

Training Challenges: While Transformers perform well on some tasks, they may require finer tuning and more data to avoid overfitting.

Overall, adding Transformers to ResNet101 can help the model capture richer and more complex patterns. However, it also introduces additional computational and memory costs, as well as possible training challenges. These tradeoffs vary from task to task and dataset to dataset, so it is best to experiment in specific application scenarios to determine the best strategy.

The Accuracy of add Transformer is 98.26%

Add transformer:

![image](https://github.com/Alen-Xue/House_damage_detect/assets/126217366/acffc49b-0382-41ed-a4ec-71c3acfdbe0d)


