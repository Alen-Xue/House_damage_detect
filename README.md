# House_damage_detect

the datasets download address is: https://www.kaggle.com/datasets/kmader/satellite-images-of-hurricane-damage

Just use your account of Kaggle to download

This code use Pytorch program library to complete this project, which include:
--
import torch

from torchvision import transforms, datasets, models

import torch.nn as nn

import numpy as np

import cv2

import pandas as pd

from PIL import Image

from torch.utils.data import Dataset, DataLoader

import matplotlib.pyplot as plt

Prepare for train and val datasets
--

In order to make the model more generalized, we need to preprocess the image, which includes random cropping, horizontal flipping, and rotation.

Use the ResNet50 as the model
--
We use the pretained model, and take careful for freeze the model weight.

Futhermore, the target of this project is to detect if 'House' in the picture is damaged, it is essentially a binary classfication model. Thus we should modify the number of feature in "fn" layer 2048->512->2(dim = 1), and to avoid overfitting set drop node rate is 20%.

Use the Grad-CAM to create damage heatmap
--
![image](https://github.com/Alen-Xue/House_damage_detect/assets/126217366/9d84d4ca-2f72-41fe-8987-e9c2ca44b72f)

Add the damge score:

The more severe the damage, the lower the score, and conversely the higher the score

![image](https://github.com/Alen-Xue/House_damage_detect/assets/126217366/b5150739-c632-4636-8cda-64c941967b90)


Grad-CAM (Gradient-weighted Class Activation Mapping) is a technique that provides insight into the reasons behind neural network decisions. Specifically, it generates a heatmap of “class-discriminative” regions, allowing us to visually identify which parts of an image contribute the most (or least) to a given output classification.

For a task such as detecting damage in pictures, Grad-CAM can highlight areas that the model finds most indicative of damage. Here’s a step-by-step explanation of how it works:

	1.	Forward Pass: An image is passed through the neural network to obtain a prediction.
	2.	Target Layer: You select a layer from the network, typically a convolutional layer. This layer contains spatial information about the image. The deeper the layer, the higher the level of abstraction it will represent.
	3.	Compute Gradients: Perform a backward pass from the predicted class score (or from any chosen output neuron) to the selected layer’s output. The goal is to compute the gradients of the predicted class score with respect to the feature maps of our chosen layer.
	4.	Global Average Pooling (GAP): The gradients computed from the previous step represent the importance of each feature map for the predicted class. By performing GAP on these gradients, you get a series of weight coefficients (one for each feature map).
	5.	Weighted Combination: Multiply each feature map of the selected layer by its associated weight coefficient from the previous step.
	6.	ReLU Activation: Take the weighted combination from the previous step and pass it through a ReLU activation function. This ensures that you consider only the features that have a positive influence on the predicted class, effectively removing negative pixels from the heatmap.
	7.	Upsampling: Resize the heatmap to the size of the input image.
	8.	Overlay on Original Image: For visualization purposes, overlay the heatmap on the original image. The resulting image highlights regions where the network thinks the damage exists.

In the context of damage detection:

	•	If the model has been trained effectively to identify damage in pictures, the heatmap generated by Grad-CAM should overlay the damage areas, suggesting that these regions highly influenced the model’s decision.
	•	The more intense the color in the heatmap (typically shown in red), the higher the model’s confidence that the corresponding area has damage.

Remember that while Grad-CAM can give an intuitive understanding of which areas the model is focusing on, it doesn’t provide an absolute measure of the model’s accuracy or reliability. If the model itself isn’t accurate or if it has been trained on poorly labeled data, the Grad-CAM visualizations might not correctly highlight damaged areas.

Use deeper layer model ResNet101 and add attention function.
--
Merge the ResNet101 and attention function (SE-block) can make model pay more attention on the feature areas.

After add attention function:
![image](https://github.com/Alen-Xue/House_damage_detect/assets/126217366/18e11cce-3e44-4feb-8130-1268d1a17d2a)





